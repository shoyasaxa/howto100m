{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function uses to download the M2E2 data from google drive onto the server \n",
    "\n",
    "def download_data():\n",
    "    zip_file_ids = [\n",
    "        (100,\"1AX3NLB4MyvaTnxKv_8VwMrUlSo0177AN\"),\n",
    "        (200,\"1bbcaf8CQQyhMsovXnBnHiW6z0xhNdaKm\"),\n",
    "        (300,\"1xbPB4CG39PWX7_EBxguocu9yGczucABc\"),\n",
    "        (400,\"1CUhhqQzs5tE-LYwVoYXtlRYR4Jmo30nu\"),\n",
    "        (500,\"1Nu7WN62TNYlxzNSLFkZr35OpckYzBSqX\"),\n",
    "        (600,\"1_dQ18MwNeMyU328CqrA3Swu1BXkIYUTM\"),\n",
    "        (700,\"1N1P2H7yGAr39PdJwAanSj0siIYTjGtO4\"),\n",
    "        (800,\"1yGlkuA_CAn2p8R7F4qqqj2q52JN8anxF\"),\n",
    "        (900,\"1OhG9CXjYUxO_JiRF9O9Xj2Fw3I4saxnZ\"),\n",
    "        (1000,\"1amrE6ATILt3Y1xxXFRbccSpbFNsyeMs5\"),\n",
    "        (1100,\"1cgalC55A6spWM4d0k0Q3GPMaOBqqvBX9\"),\n",
    "        (1200,\"1iDL2eHAj0q79Gi1_hrF6-pu6f0EfXae3\"),\n",
    "        (1300,\"169mPKJS8XFLpYHibPAjx5UckQ6ctRi8U\"),\n",
    "        (1400,\"1v9VHLG6hc6Ug3t4bK10vGmvjhhzdTnrz\"),\n",
    "        (1500,\"1eS3E1n_TsP4glsnHi0EYbAy5uobu6XAj\")\n",
    "    ]\n",
    "    \n",
    "    for zip_file_name, zip_file_id in zip_file_ids: \n",
    "        os.system(\"gdown https://drive.google.com/uc?id={}\".format(zip_file_id))\n",
    "        os.system(\"unzip {}.zip\".format(zip_file_name))\n",
    "        os.system(\"rm {}.zip\".format(zip_file_name))\n",
    "        os.system(\"mv /kiwi-data/users/shoya/AIDA/m2e2_data/{}/* /kiwi-data/users/shoya/AIDA/m2e2_data\".format(zip_file_name))\n",
    "        os.system(\"rm -rf /kiwi-data/users/shoya/AIDA/m2e2_data/{}\".format(zip_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function get a csv of absolute paths for video and feature paths \n",
    "# The CSV file is then used as an input in the howto100m feature extraction\n",
    "\n",
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "def get_input_csv_for_feature_extraction(video_path,output_folder,dimension='2D'):\n",
    "    df = pd.DataFrame()\n",
    "    video_files = [s for s in os.listdir(video_path) if s.endswith('.mp4')]\n",
    "    npy_file_names = [v.replace(\".mp4\",\".npy\") for v in video_files]\n",
    "    \n",
    "    abs_video_paths = [os.path.join(video_path,v) for v in video_files]\n",
    "    abs_npy_paths = [os.path.join(output_folder,n) for n in npy_file_names]\n",
    "    \n",
    "    df['video_path'] = abs_video_paths \n",
    "    df['feature_path'] = abs_npy_paths \n",
    "    \n",
    "    df.to_csv(\"video_input_paths_{}.csv\".format(dimension),index=False)\n",
    "    \n",
    "    print(\"Got {} rows\".format(df.shape[0]))\n",
    "    \n",
    "    \n",
    "# get_input_csv_for_feature_extraction(\"/kiwi-data/users/shoya/AIDA/m2e2_clips\",\"/kiwi-data/users/shoya/AIDA/howto100m_video_features/2d\",dimension='2D')\n",
    "# get_input_csv_for_feature_extraction(\"/kiwi-data/users/shoya/AIDA/m2e2_clips\",\"/kiwi-data/users/shoya/AIDA/howto100m_video_features/3d\",dimension='3D')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to create CSV files to be used in training script \n",
    "# For each video name, we get the corresponding file paths for the 2D and 3D video features. \n",
    "\n",
    "import pandas as pd \n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "import json \n",
    "\n",
    "def format_dataset(clips_folder_path, features_folder_path,sentences_path):\n",
    "    df = pd.DataFrame()\n",
    "    valid_videos = list(json.load(open(sentences_path)).keys())\n",
    "    video_files = [s for s in os.listdir(clips_folder_path) if s.endswith('.mp4') and s in valid_videos] \n",
    "    paths_2d = [os.path.join(features_folder_path,'2d',v.replace('.mp4','.npy')) for v in video_files]\n",
    "    paths_3d = [os.path.join(features_folder_path,'3d',v.replace('.mp4','.npy')) for v in video_files]\n",
    "    df['video_id'] = video_files\n",
    "    df['2d'] = paths_2d\n",
    "    df['3d'] = paths_3d\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    train_df.to_csv('data_paths_train.csv',index=False)\n",
    "    test_df.to_csv('data_paths_test.csv',index=False)\n",
    "    \n",
    "    \n",
    "format_dataset(\"/kiwi-data/users/shoya/AIDA/m2e2_clips\",\"/kiwi-data/users/shoya/AIDA/howto100m_video_features\",'/kiwi-data/users/shoya/AIDA/event_occurences_video_and_text_pairs.json ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import re \n",
    "\n",
    "\n",
    "# Function get the corresponding sentence given a video\n",
    "def get_corresponding_sentence(video_name,output_file_name,event_coreferences,video_clip_text_pair,data_dir):\n",
    "    text_file = open(os.path.join(data_dir,video_name.replace('.mp4','.txt'))).read() \n",
    "    coreference_texts = [] \n",
    "    for coreference_num in event_coreferences:   \n",
    "        if coreference_num > 0:\n",
    "            up_to_event = text_file[:text_file.find(\"Event {}\".format(coreference_num-1))] # OFFSET BY ONE???? \n",
    "            after_sentence = up_to_event[up_to_event.rfind(\"Sentence:\\n\") + len(\"Sentence:\\n\"):]\n",
    "\n",
    "            next_event_start = re.search(\"Event [0-9]+\", after_sentence)\n",
    "            if next_event_start is not None:\n",
    "                corefer_sentence = after_sentence[:re.search(\"Event [0-9]+\", after_sentence).start()].strip()\n",
    "            else:\n",
    "                corefer_sentence = after_sentence.strip()\n",
    "            coreference_texts.append(corefer_sentence)\n",
    "    if coreference_texts != []:\n",
    "        video_clip_text_pair[output_file_name] = coreference_texts\n",
    "    return video_clip_text_pair     \n",
    " \n",
    "# function to create a video clip from the whole video\n",
    "def clip_video(video_name,data_dir,output_data_dir,output_file_name,clip_start,clip_end):\n",
    "    os.system(\"ffmpeg -ss {} -i '{}' -to {} '{}'\".format(clip_start,os.path.join(data_dir,video_name), clip_end-clip_start, os.path.join(output_data_dir,output_file_name)))\n",
    "\n",
    "# a function to 1) clip videos and 2) get the video-sentence pairs saved into a JSON file    \n",
    "def make_video_clips(data_dir,output_data_dir,master_json_path):\n",
    "    master_json = json.load(open(master_json_path))\n",
    "    total_vids = 0 \n",
    "    event_cooccurences = 0 \n",
    "    video_clip_text_pair = {}\n",
    "    for video_name, annotations in master_json.items():\n",
    "        for annotation in annotations:\n",
    "            event_coreferences = set()\n",
    "            for key_frame in annotation['Key_Frames']:\n",
    "                for obj in key_frame['Arguments']: \n",
    "                    if obj['Event_Coreference'] is not None:\n",
    "                        event_coreferences.add(obj['Event_Coreference'])\n",
    "                    \n",
    "            if len(event_coreferences) > 0:\n",
    "                clip_start, clip_end = annotation['Temporal_Boundary'][0],annotation['Temporal_Boundary'][1]\n",
    "                output_file_name = video_name.replace('.mp4','') + ' clipped_{}_{}.mp4'.format(clip_start,clip_end)\n",
    "                print(output_file_name)\n",
    "                video_clip_text_pair = get_corresponding_sentence(video_name,output_file_name,event_coreferences,video_clip_text_pair,data_dir)\n",
    "                clip_video(video_name,data_dir,output_data_dir,output_file_name,clip_start,clip_end)\n",
    "                event_cooccurences += 1 \n",
    "                \n",
    "        total_vids += 1 \n",
    "                \n",
    "    print(\"{} total videos, {} co-occurences\".format(total_vids,event_cooccurences))\n",
    "    \n",
    "    # save mapping \n",
    "    with open('event_occurences_video_and_text_pairs.json', 'w') as f:\n",
    "        json.dump(video_clip_text_pair, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_video_clips('/Users/shoya.yoshida/AIDA/data_labeling/data/1200','/Users/shoya.yoshida/AIDA/Coreference/clipped_output','/Users/shoya.yoshida/AIDA/data_labeling/master.json')\n",
    "\n",
    "# make_video_clips('/kiwi-data/users/shoya/AIDA/m2e2_data','/kiwi-data/users/shoya/AIDA/m2e2_clips','/kiwi-data/users/shoya/AIDA/master.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_json = json.load(open('/Users/shoya.yoshida/AIDA/data_labeling/master.json'))\n",
    "# master_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master_json.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
